# v1.01: Week One

**January 14, 2026**

Buckley 1.01 ships today with substantial improvements across the stack. MCP integration, multi-provider prompt caching, a composable middleware architecture, and a suite of new commands for working with conversation history.

## MCP Tool Integration

Buckley now speaks MCP natively. External tool servers integrate seamlessly alongside builtins — custom tools appear in the same session as bash and file operations.

Why this matters: tool ecosystems shouldn't be walled gardens. You have a database connector, an API client, a domain-specific automation? Write it once as an MCP server, use it everywhere. Buckley becomes the orchestration layer for whatever you build.

## Multi-Provider Prompt Caching

Every model provider implements caching differently. Anthropic uses `cache_control` blocks. OpenAI wants explicit keys and retention policies. Others embed hints in message metadata.

Buckley detects the provider and applies the right strategy automatically. System prompts stay cached across turns without you thinking about it. Long sessions see 30-40% cost reduction — hundreds of dollars saved over a month of heavy use.

## Middleware Architecture

Tool execution is now a composable pipeline with real guarantees:

**Panic recovery** isolates tool failures. A misbehaving tool crashes its execution, not your session. Your conversation continues.

**Timeout enforcement** prevents runaway operations. Set limits per tool or globally. When something hangs, it dies cleanly.

**Retry with exponential backoff** handles transient failures. Network hiccups, rate limits, temporary outages — the system recovers without intervention.

**Approval gating** puts you in control of destructive operations. File deletions, system modifications, external API calls — confirmation required before execution.

**Pre/post hooks** open the pipeline for your own logic. Log every tool call. Transform parameters. Inject context. Build audit trails.

## Search, Export, Import

Your conversation history is data. Now you can use it.

`/search` finds past conversations by meaning or keywords. Semantic search uses embeddings to find conceptually similar discussions. Full-text search handles exact matches. That solution you worked out three weeks ago? Retrievable in seconds.

`/export` serializes sessions to JSON, Markdown, or HTML. Archive completed work. Share context with teammates. Feed conversations into other tools for analysis.

`/import` restores them. Pick up where you left off. Load a colleague's session to understand their approach. Migrate between machines.

## Context Budget Management

Long sessions inevitably approach context limits. The question is what happens when they do.

Buckley tracks token budgets continuously and trims intelligently when needed. Older context gets compacted into summaries. Recent exchanges stay intact. Tool call pairs remain coherent — no orphaned results or dangling invocations.

Cost alerting is built in. Set spend thresholds, receive notifications before surprises. Know what a session costs while you're in it.

## RLM Runtime

The recursive language model runtime handles complex multi-step work:

**Parallel tool execution** runs independent operations concurrently with configurable limits. A task that needs five file reads doesn't wait for them sequentially.

**Tool selection caching** remembers what worked. Similar requests reuse proven tool choices instead of re-asking the model every time.

**History compaction** keeps context focused. Older iterations compress into summaries while recent work stays detailed.

**RAG retrieval** brings relevant context forward. Embeddings index your scratchpad; queries surface what matters.

## TUI Enhancements

Performance work: style caching eliminates redundant allocations, dirty tracking ensures only changed regions redraw.

Interaction improvements: sidebar scrolls with mouse wheel, sections collapse on click, plan progress updates live.

## Ralph

You know that famous infinite bash loop for agentic coding? Buckley's `ralph` command is our take on it — same philosophy of endless exploration, but with the hooks, persistence, and observability you'd expect from a proper implementation. Ralph Wiggum meets Buckley: let it wander, but keep the receipts.

Define a goal, Ralph works through the steps — planning, executing, validating — without waiting for approval at every turn. Same model routing, same tool access, full session persistence. Useful for batch operations, overnight runs, or any task where you want results more than oversight.

## What This Enables

Buckley is infrastructure for working with AI models. This release strengthens that foundation.

Sessions survive crashes and resume cleanly. Costs stay predictable with caching and budgets. Tools extend infinitely through MCP. History becomes searchable, exportable, portable.

The goal isn't a clever demo. It's a tool you can rely on for real work, every day.

---

```bash
go install github.com/odvcencio/buckley/cmd/buckley@latest
```

[Changelog](https://github.com/odvcencio/buckley/compare/v1.0.0...v1.0.1) · [Source](https://github.com/odvcencio/buckley)
